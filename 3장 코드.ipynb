{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(preactivation):\n",
    "    activation = np.clip(preactivation, 0, None)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-5,5,0.1)\n",
    "ReLU_z = ReLU(z)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.plot(z, ReLU_z, 'r-')\n",
    "ax.set_xlim([-5,5]) ; ax.set_ylim([-5,5])\n",
    "ax.set_xlabel('z') ; ax.set_ylabel('ReLU[z]')\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_1_1_3(x, activation_fn, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
    "\n",
    "    pre_1 = (theta_10 + theta_11 * x)\n",
    "    pre_2 = (theta_20 + theta_21 * x)\n",
    "    pre_3 = (theta_30 + theta_31 * x)\n",
    "\n",
    "    act_1 = activation_fn(pre_1)\n",
    "    act_2 = activation_fn(pre_2)\n",
    "    act_3 = activation_fn(pre_3)\n",
    "\n",
    "    w_act_1 = (phi_1 * act_1)\n",
    "    w_act_2 = (phi_2 * act_2)\n",
    "    w_act_3 = (phi_3 * act_3)\n",
    "\n",
    "    y = ( phi_0 + w_act_1 + w_act_2 + w_act_3)\n",
    "\n",
    "    return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all = False, x_data = None, y_data = None):\n",
    "\n",
    "    if plot_all:\n",
    "        \n",
    "        fig,ax = plt.subplots(3,3)\n",
    "        fig.set_size_inches(8.5, 8.5)\n",
    "        fig.tight_layout(pad=3.0)\n",
    "\n",
    "        ax[0,0].plot(x, pre_1, 'r-') ; ax[0,0].set_ylabel('Preactivation')\n",
    "        ax[0,1].plot(x, pre_2, 'b-') ; ax[0,1].set_ylabel('Preactivation')\n",
    "        ax[0,2].plot(x, pre_3, 'g-') ; ax[0,2].set_ylabel('Preactivation')\n",
    "\n",
    "        ax[1,0].plot(x, act_1, 'r-') ; ax[1,0].set_ylabel('Activation')\n",
    "        ax[1,1].plot(x, act_2, 'b-') ; ax[1,1].set_ylabel('Activation')\n",
    "        ax[1,2].plot(x, act_3, 'g-') ; ax[1,2].set_ylabel('Activation')\n",
    "\n",
    "        ax[2,0].plot(x, w_act_1, 'r-') ; ax[2,0].set_ylabel('Weighted Act')\n",
    "        ax[2,1].plot(x, w_act_2, 'b-') ; ax[2,1].set_ylabel('Weighted Act')\n",
    "        ax[2,2].plot(x, w_act_3, 'g-') ; ax[2,2].set_ylabel('Weighted Act')\n",
    "\n",
    "    for plot_y in range(3):\n",
    "        for plot_x in range(3):\n",
    "            ax[plot_y, plot_x].set_xlim([0,1]) ; ax[plot_x, plot_y].set_ylim([-1,1])\n",
    "            ax[plot_y, plot_x].set_aspect(0.5)\n",
    "        ax[2, plot_y].set_xlabel('Input, $x$');\n",
    "    plt.show()\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "\n",
    "    ax.plot(x,y)\n",
    "    ax.set_xlabel('Input, $x$') ; ax.set_ylabel('Output, $y$')\n",
    "    ax.set_xlim([0,1]) ; ax.set_ylim([-1,1])\n",
    "    ax.set_aspect(0.5)\n",
    "\n",
    "    if x_data is not None:\n",
    "        ax.plot(x_data, y_data, 'mo')\n",
    "        for i in range(len(x_data)):\n",
    "            ax.plot(x_data[i], y_data[i],)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_10 = 0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0 ; theta_21 = 2.0\n",
    "theta_30 = -0.5 ; theta_31 = 0.65\n",
    "\n",
    "phi_0 = -0.3 ; phi_1 = 2.0 ; phi_2 = -1.0 ; phi_3 = 7.0\n",
    "\n",
    "x = np.arange(0, 1, 0.01)\n",
    "\n",
    "y, pre_1, pre_2, per_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = shallow_1_1_3(x, ReLU, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "\n",
    "plot_neural(x, y, pre_1, pre_2, per_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_10 = 0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0 ; theta_21 = 2.0\n",
    "theta_30 = -0.5 ; theta_31 = 0.65\n",
    "\n",
    "phi_0 = -0.3 ; phi_1 = 2.0 ; phi_2 = -1.0 ; phi_3 = 7.0\n",
    "\n",
    "x = np.arange(0, 1, 0.01)\n",
    "\n",
    "y, pre_1, pre_2, per_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = shallow_1_1_3(x, ReLU, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "\n",
    "plot_neural(x, y, pre_1, pre_2, per_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_loss(y_train, y_predict):\n",
    "    loss = np.sum((y_train - y_predict) ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_10 = 0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0 ; theta_21 = 2.0\n",
    "theta_30 = -0.5 ; theta_31 = 0.65\n",
    "\n",
    "phi_0 = -0.3 ; phi_1 = 2.0 ; phi_2 = -1.0 ; phi_3 = 7.0\n",
    "\n",
    "x = np.arange(0, 1, 0.01)\n",
    "\n",
    "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339, 0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\n",
    "0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,0.87168699,0.58858043])\n",
    "\n",
    "y_train = np.array([-0.15934537,0.18195445,0.451270150,0.13921448,0.09366691,0.30567674,\n",
    "0.372291170,0.40716968,-0.08131792,0.41187806,0.36943738,0.3994327,\n",
    "0.019062570,0.35820410,0.452564960,-0.0183121,0.02957665,-0.24354444, 0.148038840,0.26824970])\n",
    "\n",
    "y, pre_1, pre_2, per_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = shallow_1_1_3(x, ReLU, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "\n",
    "plot_neural(x, y, pre_1, pre_2, per_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True, x_data=x_train, y_data=y_train)\n",
    "\n",
    "y_predict, *_ = shallow_1_1_3(x_train, ReLU, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "\n",
    "loss = least_squares_loss(y_train, y_predict)\n",
    "print('Your loss = %3.3f, True value = 9.385'%(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_2D_function(ax, x1_mesh, x2_mesh, y):\n",
    "    pos = ax.contourf(x1_mesh, x2_mesh, y, levels=256, cmap='hot', vmin=-10, vmax=10.0)\n",
    "    ax.set_xlabel('x1') ; ax.set_ylabel('x2')\n",
    "    levels = np.arange(-10, 10, 1.0)\n",
    "    ax.contour(x1_mesh, x2_mesh, y, levels, cmap='winter')\n",
    "\n",
    "def plot_neural_2_inputs(x1, x2, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3):\n",
    "\n",
    "    fig,ax = plt.subplots(3,3)\n",
    "    fig.set_size_inches(8.5, 8.5)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "\n",
    "    draw_2D_function(ax[0,0], x1,x2,pre_1); ax[0,0].set_title('Preactivation')\n",
    "    draw_2D_function(ax[0,1], x1,x2,pre_2); ax[0,1].set_title('Preactivation')\n",
    "    draw_2D_function(ax[0,2], x1,x2,pre_3); ax[0,2].set_title('Preactivation')\n",
    "    \n",
    "    draw_2D_function(ax[1,0], x1,x2,act_1); ax[1,0].set_title('Activation')\n",
    "    draw_2D_function(ax[1,1], x1,x2,act_2); ax[1,1].set_title('Activation')\n",
    "    draw_2D_function(ax[1,2], x1,x2,act_3); ax[1,2].set_title('Activation')\n",
    "    \n",
    "    draw_2D_function(ax[2,0], x1,x2,w_act_1); ax[2,0].set_title('Weighted Act')\n",
    "    draw_2D_function(ax[2,1], x1,x2,w_act_2); ax[2,1].set_title('Weighted Act')\n",
    "    draw_2D_function(ax[2,2], x1,x2,w_act_3); ax[2,2].set_title('Weighted Act')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    draw_2D_function(ax, x1, x2, y)\n",
    "    ax.set_title('Network output, $y$')\n",
    "    ax.set_aspect(1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(preactivation):\n",
    "  activation = preactivation.clip(0.0)\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a shallow neural network with, two input, one output, and three hidden units\n",
    "def shallow_2_1_3(x1,x2, activation_fn, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32):\n",
    "  # TODO Replace the lines below to compute the three initial linear functions\n",
    "  # (figure 3.8a-c) from the theta parameters.  These are the preactivations\n",
    "    pre_1 = (theta_10 + theta_11 * x1 + theta_12 * x2)\n",
    "    pre_2 = (theta_20 + theta_21 * x1 + theta_22 * x2)\n",
    "    pre_3 = (theta_30 + theta_31 * x1 + theta_32 * x2)\n",
    "\n",
    "    act_1 = activation_fn(pre_1)\n",
    "    act_2 = activation_fn(pre_2)\n",
    "    act_3 = activation_fn(pre_3)\n",
    "\n",
    "    w_act_1 = (phi_1 * act_1)\n",
    "    w_act_2 = (phi_2 * act_2)\n",
    "    w_act_3 = (phi_3 * act_3)\n",
    "\n",
    "    y = ( phi_0 + w_act_1 + w_act_2 + w_act_3)\n",
    "\n",
    "    return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define some parameters and run the neural network\n",
    "theta_10 =  -4.0 ;  theta_11 = 0.9; theta_12 = 0.0\n",
    "theta_20 =  5.0  ; theta_21 = -0.9 ; theta_22 = -0.5\n",
    "theta_30 =  -7  ; theta_31 = 0.5; theta_32 = 0.9\n",
    "phi_0 = 0.0; phi_1 = -2.0; phi_2 = 2.0; phi_3 = 1.5\n",
    "\n",
    "x1 = np.arange(0.0, 10.0, 0.1)\n",
    "x2 = np.arange(0.0, 10.0, 0.1)\n",
    "x1,x2 = np.meshgrid(x1,x2)  # https://www.geeksforgeeks.org/numpy-meshgrid-function/\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = shallow_2_1_3(x1,x2, ReLU, phi_0, phi_1,phi_2, phi_3, theta_10, theta_11, theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32)\n",
    "# And then plot it\n",
    "plot_neural_2_inputs(x1,x2, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the shallow neural network.  We'll assume input in is range [0,10],[0,10] and output [-10,10]\n",
    "def plot_neural_2_inputs_2_outputs(x1,x2, y1, y2, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_11, w_act_12, w_act_13, w_act_21, w_act_22, w_act_23):\n",
    "\n",
    "  # Plot intermediate plots if flag set\n",
    "  fig, ax = plt.subplots(4,3)\n",
    "  fig.set_size_inches(8.5, 8.5)\n",
    "  fig.tight_layout(pad=3.0)\n",
    "  draw_2D_function(ax[0,0], x1,x2,pre_1); ax[0,0].set_title('Preactivation')\n",
    "  draw_2D_function(ax[0,1], x1,x2,pre_2); ax[0,1].set_title('Preactivation')\n",
    "  draw_2D_function(ax[0,2], x1,x2,pre_3); ax[0,2].set_title('Preactivation')\n",
    "  draw_2D_function(ax[1,0], x1,x2,act_1); ax[1,0].set_title('Activation')\n",
    "  draw_2D_function(ax[1,1], x1,x2,act_2); ax[1,1].set_title('Activation')\n",
    "  draw_2D_function(ax[1,2], x1,x2,act_3); ax[1,2].set_title('Activation')\n",
    "  draw_2D_function(ax[2,0], x1,x2,w_act_11); ax[2,0].set_title('Weighted Act 1')\n",
    "  draw_2D_function(ax[2,1], x1,x2,w_act_12); ax[2,1].set_title('Weighted Act 1')\n",
    "  draw_2D_function(ax[2,2], x1,x2,w_act_13); ax[2,2].set_title('Weighted Act 1')\n",
    "  draw_2D_function(ax[3,0], x1,x2,w_act_21); ax[3,0].set_title('Weighted Act 2')\n",
    "  draw_2D_function(ax[3,1], x1,x2,w_act_22); ax[3,1].set_title('Weighted Act 2')\n",
    "  draw_2D_function(ax[3,2], x1,x2,w_act_23); ax[3,2].set_title('Weighted Act 2')\n",
    "  plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  draw_2D_function(ax,x1,x2,y1)\n",
    "  ax.set_title('Network output, $y_1$')\n",
    "  ax.set_aspect(1.0)\n",
    "  plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  draw_2D_function(ax,x1,x2,y2)\n",
    "  ax.set_title('Network output, $y_2$')\n",
    "  ax.set_aspect(1.0)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a shallow neural network with, two inputs, two outputs, and three hidden units\n",
    "def shallow_2_2_3(x1,x2, activation_fn, phi_10,phi_11,phi_12,phi_13, phi_20,phi_21,phi_22,phi_23, theta_10, theta_11,\\\n",
    "                  theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32):\n",
    "\n",
    "  # TODO -- write this function -- replace the dummy code below\n",
    "  pre_1 = (theta_10 + theta_11 * x1 + theta_12 * x2)\n",
    "  pre_2 = (theta_20 + theta_21 * x1 + theta_22 * x2)\n",
    "  pre_3 = (theta_30 + theta_31 * x1 + theta_32 * x2)\n",
    "\n",
    "  act_1 = activation_fn(pre_1)\n",
    "  act_2 = activation_fn(pre_2)\n",
    "  act_3 = activation_fn(pre_3)\n",
    "\n",
    "  w_act_11 = phi_11 * act_1\n",
    "  w_act_12 = phi_12 * act_2\n",
    "  w_act_13 = phi_13 * act_3\n",
    "  w_act_21 = phi_21 * act_1\n",
    "  w_act_22 = phi_22 * act_2\n",
    "  w_act_23 = phi_23 * act_3\n",
    "  \n",
    "  y1 = phi_10 + w_act_11 + w_act_12 + w_act_13\n",
    "  y2 = phi_20 + w_act_21 + w_act_22 + w_act_23\n",
    "\n",
    "\n",
    "  # Return everything we have calculated\n",
    "  return y1,y2, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_11, w_act_12, w_act_13, w_act_21, w_act_22, w_act_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define some parameters and run the neural network\n",
    "theta_10 =  -4.0 ;  theta_11 = 0.9; theta_12 = 0.0\n",
    "theta_20 =  5.0  ; theta_21 = -0.9 ; theta_22 = -0.5\n",
    "theta_30 =  -7  ; theta_31 = 0.5; theta_32 = 0.9\n",
    "phi_10 = 0.0; phi_11 = -2.0; phi_12 = 2.0; phi_13 = 1.5\n",
    "phi_20 = -2.0; phi_21 = -1.0; phi_22 = -2.0; phi_23 = 0.8\n",
    "\n",
    "x1 = np.arange(0.0, 10.0, 0.1)\n",
    "x2 = np.arange(0.0, 10.0, 0.1)\n",
    "x1,x2 = np.meshgrid(x1,x2)  # https://www.geeksforgeeks.org/numpy-meshgrid-function/\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y1, y2, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_11, w_act_12, w_act_13, w_act_21, w_act_22, w_act_23 = \\\n",
    "    shallow_2_2_3(x1,x2, ReLU, phi_10,phi_11,phi_12,phi_13, phi_20,phi_21,phi_22,phi_23, theta_10, theta_11, theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32)\n",
    "# And then plot it\n",
    "plot_neural_2_inputs_2_outputs(x1,x2, y1, y2, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_11, w_act_12, w_act_13, w_act_21, w_act_22, w_act_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# math 매소드: https://issac-min.tistory.com/67\n",
    "# Di개의 입력과 D개의 은닉 유닛을 가지고 있을 때 생성되는 선형영역의 수\n",
    "# 이거 다시보기\n",
    "\n",
    "import math\n",
    "def number_regions(Di, D):\n",
    "  # You can use math.comb() https://www.w3schools.com/python/ref_math_comb.asp\n",
    "\n",
    "  N = sum(math.comb(D, j) for j in range(Di + 1))  # (0~Di) range는 마지막 값 표시 안함 -> +1\n",
    "  # math.comb(n, k): nCk과 같은 조합 값을 반환\n",
    "\n",
    "  return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = number_regions(2,3)\n",
    "print(f\"Di=2, D=3, Number of regions = {int(N)}, True value = 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of regions for 10D input (Di=10) and 50 hidden units (D=50)\n",
    "N = number_regions(10, 50)\n",
    "print(f\"Di=10, D=50, Number of regions = {int(N)}, True value = 13432735556\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    N = number_regions(10, 8)\n",
    "    print(f\"Di=10, D=8, Number of regions = {int(N)}, True value = 256\")\n",
    "except Exception as error:\n",
    "    print(\"An exception occurred:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the calculation properly when D<Di (see figure 3.10 from the book)\n",
    "D = 8; Di = 10\n",
    "N = np.power(2,D)\n",
    "# We can equivalently do this by calling number_regions with the D twice\n",
    "# Think about why this works\n",
    "N2 = number_regions (D,D)\n",
    "print(f\"Di=10, D=8, Number of regions = {int(N)}, Number of regions = {int(N2)}, True value = 256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = np.array([1,5,10,50,100])\n",
    "regions = np.zeros((dims.shape[0], 1000))\n",
    "\n",
    "for c_dim in range(dims.shape[0]):\n",
    "    D_i = dims[c_dim]\n",
    "    print (f\"Counting regions for {D_i} input dimensions\")\n",
    "    for D in range(1000):\n",
    "        regions[c_dim, D] = number_regions(np.min([D_i,D]), D)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.semilogy(regions[0,:],'k-')\n",
    "ax.semilogy(regions[1,:],'b-')\n",
    "ax.semilogy(regions[2,:],'m-')\n",
    "ax.semilogy(regions[3,:],'c-')\n",
    "ax.semilogy(regions[4,:],'y-')\n",
    "\n",
    "ax.legend(['$D_i$=1', '$D_i$=5', '$D_i$=10', '$D_i$=50', '$D_i$=100'])\n",
    "\n",
    "ax.set_xlabel(\"Number of hidden units, D\")\n",
    "ax.set_ylabel(\"Number of regions, N\")\n",
    "\n",
    "plt.xlim([0,1000])\n",
    "plt.ylim([1e1,1e150])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the graph from figure 3.9a (takes ~1min)\n",
    "dims = np.array([1,5,10,50,100])\n",
    "regions = np.zeros((dims.shape[0], 200))\n",
    "params = np.zeros((dims.shape[0], 200))\n",
    "\n",
    "# We'll compute the five lines separately this time to make it faster\n",
    "for c_dim in range(dims.shape[0]):\n",
    "    D_i = dims[c_dim]\n",
    "    print (f\"Counting regions for {D_i} input dimensions\")\n",
    "    for c_hidden in range(1, 200):\n",
    "        # Iterate over different ranges of number hidden variables for different input sizes\n",
    "        D = int(c_hidden * 500 / D_i)\n",
    "        params[c_dim, c_hidden] =  D_i * D +D + D +1\n",
    "        regions[c_dim, c_hidden] = number_regions(np.min([D_i,D]), D)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(params[0,:], regions[0,:],'k-')\n",
    "ax.semilogy(params[1,:], regions[1,:],'b-')\n",
    "ax.semilogy(params[2,:], regions[2,:],'m-')\n",
    "ax.semilogy(params[3,:], regions[3,:],'c-')\n",
    "ax.semilogy(params[4,:], regions[4,:],'y-')\n",
    "ax.legend(['$D_i$=1', '$D_i$=5', '$D_i$=10', '$D_i$=50', '$D_i$=100'])\n",
    "ax.set_xlabel(\"Number of parameters, D\")\n",
    "ax.set_ylabel(\"Number of regions, N\")\n",
    "plt.xlim([0,100000])\n",
    "plt.ylim([1e1,1e150])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3_4 activaition 부터 시작\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all = False, x_data=None, y_data=None):\n",
    "\n",
    "    if plot_all:\n",
    "        fig,ax = plt.subplots(3,3)\n",
    "        fig.set_size_inches(8.5, 8.5)\n",
    "        fig.tight_layout(pad=3.0)\n",
    "\n",
    "        ax[0,0].plot(x, pre_1, 'r-') ; ax[0,0].set_ylabel('Preactivation')\n",
    "        ax[0,1].plot(x, pre_2, 'b-') ; ax[0,1].set_ylabel('Preactivation')\n",
    "        ax[0,2].plot(x, pre_3, 'g-') ; ax[0,2].set_ylabel('Preactivation')\n",
    "\n",
    "        ax[1,0].plot(x, act_1, 'r-') ; ax[1,0].set_ylabel('Activation')\n",
    "        ax[1,1].plot(x, act_2, 'b-') ; ax[1,1].set_ylabel('Activation')\n",
    "        ax[1,2].plot(x, act_3, 'g-') ; ax[1,2].set_ylabel('Activation')\n",
    "\n",
    "        ax[2,0].plot(x, w_act_1, 'r-') ; ax[2,0].set_ylabel('Weighted Act')\n",
    "        ax[2,1].plot(x, w_act_2, 'b-') ; ax[2,1].set_ylabel('Weighted Act')\n",
    "        ax[2,2].plot(x, w_act_3, 'g-') ; ax[2,2].set_ylabel('Weighted Act')\n",
    "\n",
    "    for plot_y in range(3):\n",
    "        for plot_x in range(3):\n",
    "            ax[plot_y, plot_x].set_xlim([0,1]) ; ax[plot_x, plot_y].set_ylim([-1,1])\n",
    "            ax[plot_y, plot_x].set_aspect(0.5)\n",
    "        ax[2, plot_y].set_xlabel('Input, $x$');\n",
    "    plt.show()\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "\n",
    "    ax.plot(x,y)\n",
    "    ax.set_xlabel('Input, $x$') ; ax.set_ylabel('Output, $y$')\n",
    "    ax.set_xlim([0,1]) ; ax.set_ylim([-1,1])\n",
    "    ax.set_aspect(0.5)\n",
    "\n",
    "    if x_data is not None:\n",
    "        ax.plot(x_data, y_data, 'mo')\n",
    "        for i in range(len(x_data)):\n",
    "            ax.plot(x_data[i], y_data[i],)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_1_1_3(x, activation_fn, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
    "  pre_1 = theta_10 + theta_11 * x\n",
    "  pre_2 = theta_20 + theta_21 * x\n",
    "  pre_3 = theta_30 + theta_31 * x\n",
    "  # Pass these through the ReLU function to compute the activations as in\n",
    "  # figure 3.3 d-f\n",
    "  act_1 = activation_fn(pre_1)\n",
    "  act_2 = activation_fn(pre_2)\n",
    "  act_3 = activation_fn(pre_3)\n",
    "\n",
    "  w_act_1 = phi_1 * act_1\n",
    "  w_act_2 = phi_2 * act_2\n",
    "  w_act_3 = phi_3 * act_3\n",
    "\n",
    "  y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
    "\n",
    "  # Return everything we have calculated\n",
    "  return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation):\n",
    "  activation = preactivation.clip(0.0)\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define some parameters and run the neural network\n",
    "theta_10 =  0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0  ; theta_21 = 2.0\n",
    "theta_30 = -0.5  ; theta_31 = 0.65\n",
    "phi_0 = -0.3; phi_1 = 2.0; phi_2 = -1.0; phi_3 = 7.0\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "# And then plot it\n",
    "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(preactivation):\n",
    "    sigmoid = (1/1+np.exp())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
