{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(preactivation):\n",
    "    activation = preactivation.clip(0.0)\n",
    "    return activation\n",
    "\n",
    "def shallow_nn(x, beta_0, omega_0, beta_1, omega_1):\n",
    "\n",
    "    n_data = x.size\n",
    "    x = np.reshape(x, (1, n_data))\n",
    "\n",
    "    h1 = ReLU(np.matmul(beta_0, np.ones((1, n_data))) + np.matmul(omega_0, x))\n",
    "    y = np.matmul(beta_1, np.ones((1, n_data))) + np.matmul(omega_1, h1)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters():\n",
    "  # And we'll create a network that approximately fits it\n",
    "  beta_0 = np.zeros((3,1));  # formerly theta_x0\n",
    "  omega_0 = np.zeros((3,1)); # formerly theta_x1\n",
    "  beta_1 = np.zeros((1,1));  # formerly phi_0\n",
    "  omega_1 = np.zeros((1,3)); # formerly phi_x\n",
    "\n",
    "  beta_0[0,0] = 0.3; beta_0[1,0] = -1.0; beta_0[2,0] = -0.5\n",
    "  omega_0[0,0] = -1.0; omega_0[1,0] = 1.8; omega_0[2,0] = 0.65\n",
    "  beta_1[0,0] = 0.1;\n",
    "  omega_1[0,0] = -2.0; omega_1[0,1] = -1.0; omega_1[0,2] = 7.0\n",
    "\n",
    "  return beta_0, omega_0, beta_1, omega_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for plotting data\n",
    "def plot_univariate_regression(x_model, y_model, x_data = None, y_data = None, sigma_model = None, title= None):\n",
    "  # Make sure model data are 1D arrays\n",
    "  x_model = np.squeeze(x_model)\n",
    "  y_model = np.squeeze(y_model)\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(x_model,y_model)\n",
    "\n",
    "  if sigma_model is not None:\n",
    "    ax.fill_between(x_model, y_model-2*sigma_model, y_model+2*sigma_model, color='lightgray')\n",
    "    ax.set_xlabel(r'Input, $x$'); ax.set_ylabel(r'Output, $y$')\n",
    "    ax.set_xlim([0,1]);ax.set_ylim([-1,1])\n",
    "    ax.set_aspect(0.5)\n",
    "\n",
    "  if title is not None:\n",
    "    ax.set_title(title)\n",
    "\n",
    "  if x_data is not None:\n",
    "    ax.plot(x_data, y_data, 'ko')\n",
    "    \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some 1D training data\n",
    "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339,\\\n",
    "                   0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\\\n",
    "                   0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,\\\n",
    "                   0.87168699,0.58858043])\n",
    "y_train = np.array([-0.25934537,0.18195445,0.651270150,0.13921448,0.09366691,0.30567674,\\\n",
    "                    0.372291170,0.20716968,-0.08131792,0.51187806,0.16943738,0.3994327,\\\n",
    "                    0.019062570,0.55820410,0.452564960,-0.1183121,0.02957665,-1.24354444, \\\n",
    "                    0.248038840,0.26824970])\n",
    "\n",
    "# Get parameters for the model\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "sigma = 0.2\n",
    "\n",
    "# Define a range of input values\n",
    "x_model = np.arange(0,1,0.01)\n",
    "# Run the model to get values to plot and plot it.\n",
    "y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_distribution(y, mu, sigma):\n",
    "\n",
    "    exponet = -(y-mu) ** 2 / (2 * (sigma) ** 2)\n",
    "    prob = ( np.exp(exponet) / (sigma * np.sqrt(2 * np.pi)))\n",
    "\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %3.3f, Your answer = %3.3f\"%(0.119,normal_distribution(1,-1,2.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the Gaussian distribution.\n",
    "y_gauss = np.arange(-5,5,0.1)\n",
    "\n",
    "mu = 0; sigma = 1.0\n",
    "\n",
    "gauss_prob = normal_distribution(y_gauss, mu, sigma)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_gauss, gauss_prob)\n",
    "ax.set_xlabel(r'Input, $y$'); ax.set_ylabel(r'Probability $Pr(y)$')\n",
    "ax.set_xlim([-5,5]);ax.set_ylim([0,1.0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_likelihood(y_train, mu, sigma):\n",
    "\n",
    "    exponet = -(y_train - mu) ** 2 / (2 * sigma ** 2)\n",
    "\n",
    "    normal_distribution = 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(exponet)\n",
    "\n",
    "    likelihood = np.prod(normal_distribution)\n",
    "    \n",
    "    return likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer = 0.000010624, Your answer = 0.000010624\n"
     ]
    }
   ],
   "source": [
    "# Let's test this for a homoscedastic (constant sigma) model\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "# Use our neural network to predict the mean of the Gaussian\n",
    "mu_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "# Set the standard deviation to something reasonable\n",
    "sigma = 0.2\n",
    "# Compute the likelihoodl\n",
    "likelihood = compute_likelihood(y_train, mu_pred, sigma)\n",
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(0.000010624,likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_negativce_loglikelihood(y_train, mu, sigma):\n",
    "\n",
    "    exponet = -(y_train-mu) ** 2 / (2 * (sigma) ** 2)\n",
    "    prob = ( np.exp(exponet) / (sigma * np.sqrt(2 * np.pi)))\n",
    "\n",
    "    nll = -np.sum(np.log(prob))\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sum_of_squares(y_train, y_pred):\n",
    "    \n",
    "    squares_distances = (y_train - y_pred) ** 2\n",
    "\n",
    "    sum_of_squares = np.sum(squares_distances)\n",
    "    \n",
    "    return sum_of_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer = 2.020992572, Your answer = 2.020992572\n"
     ]
    }
   ],
   "source": [
    "# Let's test this again\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "# Use our neural network to predict the mean of the Gaussian, which is out best prediction of y\n",
    "y_pred = mu_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "# Compute the sum of squares\n",
    "sum_of_squares = compute_sum_of_squares(y_train, y_pred)\n",
    "# Let's double check we get the right answer before proceeding\n",
    "print(\"Correct answer = %9.9f, Your answer = %9.9f\"%(2.020992572,sum_of_squares))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of values for the parameter\n",
    "beta_1_vals = np.arange(0,1.0,0.01)\n",
    "# Create some arrays to store the likelihoods, negative log likelihoods and sum of squares\n",
    "likelihoods = np.zeros_like(beta_1_vals)\n",
    "nlls = np.zeros_like(beta_1_vals)\n",
    "sum_squares = np.zeros_like(beta_1_vals)\n",
    "\n",
    "# Initialise the parameters\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "sigma = 0.2\n",
    "for count in range(len(beta_1_vals)):\n",
    "  # Set the value for the parameter\n",
    "  beta_1[0,0] = beta_1_vals[count]\n",
    "  # Run the network with new parameters\n",
    "  mu_pred = y_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "  # Compute and store the three values\n",
    "  likelihoods[count] = compute_likelihood(y_train, mu_pred, sigma)\n",
    "  nlls[count] = compute_negativce_loglikelihood(y_train, mu_pred, sigma)\n",
    "  sum_squares[count] = compute_sum_of_squares(y_train, y_pred)\n",
    "  # Draw the model for every 20th parameter setting\n",
    "  if count % 20 == 0:\n",
    "    # Run the model to get values to plot and plot it.\n",
    "    y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "    plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma, title=\"beta1=%3.3f\"%(beta_1[0,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the likelihood, negative log likelihood, and least squares as a function of the value of the offset beta1\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(10.5, 5.5)\n",
    "fig.tight_layout(pad=10.0)\n",
    "likelihood_color = 'tab:red'\n",
    "nll_color = 'tab:blue'\n",
    "\n",
    "ax[0].set_xlabel('beta_1[0]')\n",
    "ax[0].set_ylabel('likelihood', color = likelihood_color)\n",
    "ax[0].plot(beta_1_vals, likelihoods, color = likelihood_color)\n",
    "ax[0].tick_params(axis='y', labelcolor=likelihood_color)\n",
    "\n",
    "ax00 = ax[0].twinx()\n",
    "ax00.plot(beta_1_vals, nlls, color = nll_color)\n",
    "ax00.set_ylabel('negative log likelihood', color = nll_color)\n",
    "ax00.tick_params(axis='y', labelcolor = nll_color)\n",
    "\n",
    "plt.axvline(x = beta_1_vals[np.argmax(likelihoods)], linestyle='dotted')\n",
    "\n",
    "ax[1].plot(beta_1_vals, sum_squares); ax[1].set_xlabel('beta_1[0]'); ax[1].set_ylabel('sum of squares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood\n",
    "# and the least squares solutions\n",
    "# Let's check that:\n",
    "print(\"Maximum likelihood = %3.3f, at beta_1=%3.3f\"%( (likelihoods[np.argmax(likelihoods)],beta_1_vals[np.argmax(likelihoods)])))\n",
    "print(\"Minimum negative log likelihood = %3.3f, at beta_1=%3.3f\"%( (nlls[np.argmin(nlls)],beta_1_vals[np.argmin(nlls)])))\n",
    "print(\"Least squares = %3.3f, at beta_1=%3.3f\"%( (sum_squares[np.argmin(sum_squares)],beta_1_vals[np.argmin(sum_squares)])))\n",
    "\n",
    "# Plot the best model\n",
    "beta_1[0,0] = beta_1_vals[np.argmin(sum_squares)]\n",
    "y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma, title=\"beta1=%3.3f\"%(beta_1[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of values for the parameter\n",
    "sigma_vals = np.arange(0.1,0.5,0.005)\n",
    "# Create some arrays to store the likelihoods, negative log likelihoods and sum of squares\n",
    "likelihoods = np.zeros_like(sigma_vals)\n",
    "nlls = np.zeros_like(sigma_vals)\n",
    "sum_squares = np.zeros_like(sigma_vals)\n",
    "\n",
    "# Initialise the parameters\n",
    "beta_0, omega_0, beta_1, omega_1 = get_parameters()\n",
    "# Might as well set to the best offset\n",
    "beta_1[0,0] = 0.27\n",
    "for count in range(len(sigma_vals)):\n",
    "  # Set the value for the parameter\n",
    "  sigma = sigma_vals[count]\n",
    "  # Run the network with new parameters\n",
    "  mu_pred = y_pred = shallow_nn(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "  # Compute and store the three values\n",
    "  likelihoods[count] = compute_likelihood(y_train, mu_pred, sigma)\n",
    "  nlls[count] = compute_negativce_loglikelihood(y_train, mu_pred, sigma)\n",
    "  sum_squares[count] = compute_sum_of_squares(y_train, y_pred)\n",
    "  # Draw the model for every 20th parameter setting\n",
    "  if count % 20 == 0:\n",
    "    # Run the model to get values to plot and plot it.\n",
    "    y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "    plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model=sigma, title=\"sigma=%3.3f\"%(sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the likelihood, negative log likelihood, and least squares as a function of the value of the standard deviation sigma\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(10.5, 5.5)\n",
    "fig.tight_layout(pad=10.0)\n",
    "likelihood_color = 'tab:red'\n",
    "nll_color = 'tab:blue'\n",
    "\n",
    "\n",
    "ax[0].set_xlabel('sigma')\n",
    "ax[0].set_ylabel('likelihood', color = likelihood_color)\n",
    "ax[0].plot(sigma_vals, likelihoods, color = likelihood_color)\n",
    "ax[0].tick_params(axis='y', labelcolor=likelihood_color)\n",
    "\n",
    "ax00 = ax[0].twinx()\n",
    "ax00.plot(sigma_vals, nlls, color = nll_color)\n",
    "ax00.set_ylabel('negative log likelihood', color = nll_color)\n",
    "ax00.tick_params(axis='y', labelcolor = nll_color)\n",
    "\n",
    "plt.axvline(x = sigma_vals[np.argmax(likelihoods)], linestyle='dotted')\n",
    "\n",
    "ax[1].plot(sigma_vals, sum_squares); ax[1].set_xlabel('sigma'); ax[1].set_ylabel('sum of squares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hopefully, you can see that the maximum of the likelihood fn is at the same position as the minimum negative log likelihood\n",
    "# The least squares solution does not depend on sigma, so it's just flat -- no use here.\n",
    "# Let's check that:\n",
    "print(\"Maximum likelihood = %3.3f, at sigma=%3.3f\"%( (likelihoods[np.argmax(likelihoods)],sigma_vals[np.argmax(likelihoods)])))\n",
    "print(\"Minimum negative log likelihood = %3.3f, at sigma=%3.3f\"%( (nlls[np.argmin(nlls)],sigma_vals[np.argmin(nlls)])))\n",
    "# Plot the best model\n",
    "sigma= sigma_vals[np.argmin(nlls)]\n",
    "y_model = shallow_nn(x_model, beta_0, omega_0, beta_1, omega_1)\n",
    "plot_univariate_regression(x_model, y_model, x_train, y_train, sigma_model = sigma, title=\"beta_1=%3.3f, sigma =%3.3f\"%(beta_1[0,0],sigma))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
