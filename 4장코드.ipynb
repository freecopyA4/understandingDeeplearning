{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(preactivation):\n",
    "    activation = preactivation.clip(0.0)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_1_1_3(x, activation_fn, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
    "\n",
    "  pre_1 = theta_10 + theta_11 * x\n",
    "  pre_2 = theta_20 + theta_21 * x\n",
    "  pre_3 = theta_30 + theta_31 * x\n",
    "\n",
    "  act_1 = activation_fn(pre_1)\n",
    "  act_2 = activation_fn(pre_2)\n",
    "  act_3 = activation_fn(pre_3)\n",
    " \n",
    "  w_act_1 = phi_1 * act_1\n",
    "  w_act_2 = phi_2 * act_2\n",
    "  w_act_3 = phi_3 * act_3\n",
    "  \n",
    "  y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
    "  \n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neural_two_components(x_in, net1_out, net2_out, net12_out=None):\n",
    "\n",
    "  fig,ax = plt.subplots(1,2)\n",
    "  fig.set_size_inches(8.5, 8.5)\n",
    "  fig.tight_layout(pad=3.0)\n",
    "\n",
    "  ax[0].plot(x_in, net1_out,'r-')\n",
    "  ax[0].set_xlabel('Net 1 input'); ax[0].set_ylabel('Net 1 output')\n",
    "  ax[0].set_xlim([-1,1]); ax[0].set_ylim([-1,1])\n",
    "  ax[0].set_aspect(1.0)\n",
    "\n",
    "  ax[1].plot(net1_out, net2_out,'b-')\n",
    "  ax[1].set_xlabel('Net 1 output'); ax[1].set_ylabel('Net 2 output')\n",
    "  ax[1].set_xlim([-1,1]);ax[1].set_ylim([-1,1])\n",
    "  ax[1].set_aspect(1.0)\n",
    "  plt.show()\n",
    "\n",
    "  if net12_out is not None:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_in ,net12_out,'g-')\n",
    "    ax.set_xlabel('Net 1 input'); ax.set_ylabel('Net 2 output')\n",
    "    ax.set_xlim([-1,1]);ax.set_ylim([-1,1])\n",
    "    ax.set_aspect(1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1_theta_10 = 0.0   ; n1_theta_11 = -1.0\n",
    "n1_theta_20 = 0     ; n1_theta_21 = 1.0\n",
    "n1_theta_30 = -0.67 ; n1_theta_31 =  1.0\n",
    "n1_phi_0 = 1.0; n1_phi_1 = -2.0; n1_phi_2 = -3.0; n1_phi_3 = 9.3\n",
    "\n",
    "\n",
    "n2_theta_10 =  -0.6 ; n2_theta_11 = -1.0\n",
    "n2_theta_20 =  0.2  ; n2_theta_21 = 1.0\n",
    "n2_theta_30 =  -0.5  ; n2_theta_31 =  1.0\n",
    "n2_phi_0 = 0.5; n2_phi_1 = -1.0; n2_phi_2 = -1.5; n2_phi_3 = 2.0\n",
    "\n",
    "\n",
    "x = np.arange(-1, 1, 0.001)\n",
    "\n",
    "net1_out = shallow_1_1_3(x, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)\n",
    "net2_out = shallow_1_1_3(net1_out, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
    "\n",
    "plot_neural_two_components(x, net1_out, net2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net12_out = net1_out + net2_out\n",
    "\n",
    "# Plot all three graphsb\n",
    "plot_neural_two_components(x, net1_out, net2_out, net12_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net12_out = shallow_1_1_3(net1_out, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
    "\n",
    "plot_neural_two_components(x, net1_out, net2_out, net12_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(preactivation):\n",
    "    activation = preactivation.clip(0.0)\n",
    "    return activation\n",
    "\n",
    "def shallow_1_1_3_3(x, activation_fn, phi, psi, theta):\n",
    "\n",
    "    layer_pre_1 = theta[1,0] + theta[1,1] * x\n",
    "    layer_pre_2 = theta[2,0] + theta[2,1] * x\n",
    "    layer_pre_3 = theta[3,0] + theta[3,1] * x\n",
    "\n",
    "    h1 = activation_fn(layer_pre_1)\n",
    "    h2 = activation_fn(layer_pre_2)\n",
    "    h3 = activation_fn(layer_pre_3)\n",
    "\n",
    "    layer2_pre_1 = psi[1,0] + psi[1,1] * h1 + psi[1,2] * h2 + psi[1,3] * h3\n",
    "    layer2_pre_2 = psi[2,0] + psi[2,1] * h1 + psi[2,2] * h2 + psi[2,3] * h3\n",
    "    layer2_pre_3 = psi[3,0] + psi[3,1] * h1 + psi[3,2] * h2 + psi[3,3] * h3\n",
    "\n",
    "    h1_prime = activation_fn(layer2_pre_1)\n",
    "    h2_prime = activation_fn(layer2_pre_2)\n",
    "    h3_prime = activation_fn(layer2_pre_3)\n",
    "\n",
    "    phi1_h1_prime = phi[1] * h1_prime\n",
    "    phi2_h2_prime = phi[2] * h2_prime\n",
    "    phi3_h3_prime = phi[3] * h3_prime\n",
    "\n",
    "    y = phi[0] + phi1_h1_prime + phi2_h2_prime + phi3_h3_prime\n",
    "\n",
    "    return y, layer2_pre_1, layer_pre_2, layer_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neural_two_layers(x, y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime):\n",
    "\n",
    "    fig, ax = plt.subplots(3,3)\n",
    "    fig.set_size_inches(8.5, 8.5)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "\n",
    "    ax[0,0].plot(x,layer2_pre_1,'r-'); ax[0,0].set_xlabel('$\\psi_{10}+\\psi_{11}h_{1}+\\psi_{12}h_{2}+\\psi_{13}h_3$')\n",
    "    ax[0,1].plot(x,layer2_pre_2,'b-'); ax[0,1].set_xlabel('$\\psi_{20}+\\psi_{21}h_{1}+\\psi_{22}h_{2}+\\psi_{23}h_3$')\n",
    "    ax[0,2].plot(x,layer2_pre_3,'g-'); ax[0,2].set_xlabel('$\\psi_{30}+\\psi_{31}h_{1}+\\psi_{32}h_{2}+\\psi_{33}h_3$')\n",
    "\n",
    "    ax[1,0].plot(x,h1_prime,'r-'); ax[1,0].set_xlabel(\"$h_{1}^{'}$\")\n",
    "    ax[1,1].plot(x,h2_prime,'b-'); ax[1,1].set_xlabel(\"$h_{2}^{'}$\")\n",
    "    ax[1,2].plot(x,h3_prime,'g-'); ax[1,2].set_xlabel(\"$h_{3}^{'}$\")\n",
    "    \n",
    "    ax[2,0].plot(x,phi1_h1_prime,'r-'); ax[2,0].set_xlabel(\"$\\phi_1 h_{1}^{'}$\")\n",
    "    ax[2,1].plot(x,phi2_h2_prime,'b-'); ax[2,1].set_xlabel(\"$\\phi_2 h_{2}^{'}$\")\n",
    "    ax[2,2].plot(x,phi3_h3_prime,'g-'); ax[2,2].set_xlabel(\"$\\phi_3 h_{3}^{'}$\")\n",
    "\n",
    "    for plot_y in range(3):\n",
    "      for plot_x in range(3):\n",
    "        ax[plot_y,plot_x].set_xlim([0,1]);ax[plot_x,plot_y].set_ylim([-1,1])\n",
    "        ax[plot_y,plot_x].set_aspect(0.5)\n",
    "      ax[2,plot_y].set_xlabel('Input, $x$');\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x,y)\n",
    "    ax.set_xlabel('Input, $x$'); ax.set_xlabel('Output, $y$')\n",
    "    ax.set_xlim([0,1]);ax.set_ylim([-1,1])\n",
    "    ax.set_aspect(0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros([4,2])\n",
    "psi = np.zeros([4,4])\n",
    "phi = np.zeros([4,1])\n",
    "\n",
    "theta[1,0] =  0.3 ; theta[1,1] = -1.0\n",
    "theta[2,0]= -1.0  ; theta[2,1] = 2.0\n",
    "theta[3,0] = -0.5  ; theta[3,1] = 0.65\n",
    "\n",
    "psi[1,0] = 0.3;  psi[1,1] = 2.0; psi[1,2] = -1.0; psi[1,3]=7.0\n",
    "psi[2,0] = -0.2;  psi[2,1] = 2.0; psi[2,2] = 1.2; psi[2,3]=-8.0\n",
    "psi[3,0] = 0.3;  psi[3,1] = -2.3; psi[3,2] = -0.8; psi[3,3]=2.0\n",
    "\n",
    "phi[0] = 0.0; phi[1] = 0.5; phi[2] = -1.5; phi [3] = 2.2\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# Run the neural network\n",
    "y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime = shallow_1_1_3_3(x, ReLU, phi, psi, theta)\n",
    "\n",
    "# And then plot it\n",
    "plot_neural_two_layers(x, y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(preactivation):\n",
    "    activation = preactivation.clip(0,0)\n",
    "    return preactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_1_1_3(x, activation_fn, phi_0, phi_1, phi_2, phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
    "\n",
    "    pre_1 = theta_10 + theta_11 * x\n",
    "    pre_2 = theta_20 + theta_21 * x\n",
    "    pre_3 = theta_30 + theta_31 * x\n",
    "\n",
    "    act_1 = activation_fn(pre_1)\n",
    "    act_2 = activation_fn(pre_2)\n",
    "    act_3 = activation_fn(pre_3)\n",
    "\n",
    "    w_act_1 = phi_1 * act_1\n",
    "    w_act_2 = phi_2 * act_2\n",
    "    w_act_3 = phi_3 * act_3\n",
    "\n",
    "    y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
    "\n",
    "    return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neural(x,y):\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(x.T, y.T)\n",
    "    ax.set_xlabel('Input'); ax.set_ylabel('Output')\n",
    "    ax.set_xlim([-1,1]); ax.set_ylim([-1,1])\n",
    "    ax.set_aspect(1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1_theta_10 = 0.0   ; n1_theta_11 = -1.0\n",
    "n1_theta_20 = 0     ; n1_theta_21 = 1.0\n",
    "n1_theta_30 = -0.67 ; n1_theta_31 =  1.0\n",
    "n1_phi_0 = 1.0; n1_phi_1 = -2.0; n1_phi_2 = -3.0; n1_phi_3 = 9.3\n",
    "\n",
    "# Define a range of input values\n",
    "n1_in = np.arange(-1,1,0.01).reshape([1,-1])\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "n1_out, *_ = shallow_1_1_3(n1_in, ReLU, n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3, n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)\n",
    "# And then plot it\n",
    "plot_neural(n1_in, n1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = np.zeros((3,1))\n",
    "Omega_0 = np.zeros((3,1))\n",
    "beta_1 = np.zeros((1,1))\n",
    "Omega_1 = np.zeros((1,3))\n",
    "\n",
    "# TODO Fill in the values of the beta and Omega matrices with the n1_theta and n1_phi parameters that define the network above\n",
    "# !!! NOTE THAT MATRICES ARE CONVENTIONALLY INDEXED WITH a_11 IN THE TOP LEFT CORNER, BUT NDARRAYS START AT [0,0]\n",
    "# To get you started I've filled in a couple:\n",
    "\n",
    "# beta_0와 Omega_0에 값을 할당하는 코드 예시\n",
    "beta_0[0,0] = n1_theta_10; Omega_0[0,0] = n1_theta_11\n",
    "beta_0[1,0] = n1_theta_20; Omega_0[1,0] = n1_theta_21 \n",
    "beta_0[2,0] = n1_theta_30; Omega_0[2,0] = n1_theta_31 \n",
    "\n",
    "beta_1[0,0] = n1_phi_0; \n",
    "\n",
    "Omega_1[0,0] = n1_phi_1; Omega_1[0,1] = n1_phi_2; Omega_1[0,2] = n1_phi_3\n",
    "\n",
    "\n",
    "# Make sure that input data matrix has different inputs in its columns\n",
    "n_data = n1_in.size\n",
    "n_dim_in = 1\n",
    "\n",
    "n1_in_mat = np.reshape(n1_in,(n_dim_in,n_data))\n",
    "\n",
    "# This runs the network for ALL of the inputs, x at once so we can draw graph\n",
    "h1 = ReLU(np.matmul(beta_0,np.ones((1,n_data))) + np.matmul(Omega_0,n1_in_mat))\n",
    "n1_out = np.matmul(beta_1,np.ones((1,n_data))) + np.matmul(Omega_1,h1)\n",
    "\n",
    "# Draw the network and check that it looks the same as the non-matrix case\n",
    "plot_neural(n1_in, n1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define some parameters and run the second neural network\n",
    "n2_theta_10 =  -0.6 ; n2_theta_11 = -1.0\n",
    "n2_theta_20 =  0.2  ; n2_theta_21 = 1.0\n",
    "n2_theta_30 =  -0.5  ; n2_theta_31 =  1.0\n",
    "n2_phi_0 = 0.5; n2_phi_1 = -1.0; n2_phi_2 = -1.5; n2_phi_3 = 2.0\n",
    "\n",
    "# Define a range of input values\n",
    "n2_in = np.arange(-1,1,0.01)\n",
    "\n",
    "# We run the second neural network on the output of the first network\n",
    "n2_out, *_ = shallow_1_1_3(n1_out, ReLU, n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3, n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)\n",
    "# And then plot it\n",
    "plot_neural(n1_out, n2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = np.zeros((3,1))\n",
    "Omega_0 = np.zeros((3,1))\n",
    "beta_1 = np.zeros((3,1))\n",
    "Omega_1 = np.zeros((3,3))\n",
    "beta_2 = np.zeros((1,1))\n",
    "Omega_2 = np.zeros((1,3))\n",
    "\n",
    "# TODO Fill in the values of the beta and Omega matrices for the n1_theta, n1_phi, n2_theta, and n2_phi parameters\n",
    "# that define the composition of the two networks above (see eqn 4.5 for Omega1 and beta1 albeit in different notation)\n",
    "# !!! NOTE THAT MATRICES ARE CONVENTIONALLY INDEXED WITH a_11 IN THE TOP LEFT CORNER, BUT NDARRAYS START AT [0,0] SO EVERYTHING IS OFFSET\n",
    "# To get you started I've filled in a few:\n",
    "\n",
    "beta_0[0,0] = n1_theta_10\n",
    "beta_0[1,0] = n1_theta_20\n",
    "beta_0[2,0] = n1_theta_30\n",
    "\n",
    "Omega_0[0,0] = n1_theta_11\n",
    "Omega_0[1,0] = n1_theta_21 \n",
    "Omega_0[2,0] = n1_theta_31 \n",
    "\n",
    "beta_1[0,0] = n2_theta_10 + n2_theta_11 * n1_phi_0\n",
    "beta_1[1,0] = n2_theta_20 + n2_theta_21 * n1_phi_2\n",
    "beta_1[2,0] = n2_theta_30 + n2_theta_31 * n1_phi_3\n",
    "\n",
    "Omega_1[0,0] = n2_theta_11 * n1_phi_1\n",
    "Omega_1[1,0] = n2_theta_21 * n1_phi_2\n",
    "Omega_1[2,0] = n2_theta_31 * n1_phi_3\n",
    "\n",
    "beta_2[0,0] = n2_phi_0\n",
    "\n",
    "Omega_2[0,0] = n2_phi_1; Omega_2[0,1] = n2_phi_2; Omega_2[0,2] = n2_phi_3\n",
    "\n",
    "# Make sure that input data matrix has different inputs in its columns\n",
    "n_data = n1_in.size\n",
    "n_dim_in = 1\n",
    "n1_in_mat = np.reshape(n1_in,(n_dim_in,n_data))\n",
    "\n",
    "# This runs the network for ALL of the inputs, x at once so we can draw graph (hence extra np.ones term)\n",
    "h1 = ReLU(np.matmul(beta_0,np.ones((1,n_data))) + np.matmul(Omega_0,n1_in_mat))\n",
    "h2 = ReLU(np.matmul(beta_1,np.ones((1,n_data))) + np.matmul(Omega_1,h1))\n",
    "n2_out = np.matmul(beta_2,np.ones((1,n_data))) + np.matmul(Omega_2,h2)\n",
    "\n",
    "# Draw the network and check that it looks the same as the non-matrix version\n",
    "plot_neural(n1_in, n1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define sizes\n",
    "D_i = 4\n",
    "D_1 = 5\n",
    "D_2 = 2\n",
    "D_3 = 4\n",
    "D_o = 1\n",
    "n_data = 4\n",
    "\n",
    "# Initialize parameters randomly with the correct sizes\n",
    "beta_0 = np.random.normal(size=(1, D_1))\n",
    "Omega_0 = np.random.normal(size=(D_1, D_i))\n",
    "beta_1 = np.random.normal(size=(1, D_2))\n",
    "Omega_1 = np.random.normal(size=(D_2, D_1))\n",
    "beta_2 = np.random.normal(size=(1, D_3))\n",
    "Omega_2 = np.random.normal(size=(D_3, D_2))\n",
    "beta_3 = np.random.normal(size=(1, D_o))\n",
    "Omega_3 = np.random.normal(size=(D_o, D_3))\n",
    "\n",
    "# Define ReLU activation function\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Compute forward pass\n",
    "x = np.random.normal(size=(D_i, n_data))\n",
    "h1 = ReLU(np.matmul(Omega_0, x) + beta_0.T)  # Transpose beta_0 to match dimensions\n",
    "h2 = ReLU(np.matmul(Omega_1, h1) + beta_1.T)  # Transpose beta_1 to match dimensions\n",
    "h3 = ReLU(np.matmul(Omega_2, h2) + beta_2.T)  # Transpose beta_2 to match dimensions\n",
    "y = np.matmul(Omega_3, h3) + beta_3.T  # Transpose beta_3 to match dimensions\n",
    "\n",
    "# Check shapes\n",
    "if h1.shape != (D_1, n_data):\n",
    "    print(\"h1 has wrong shape\")\n",
    "if h2.shape != (D_2, n_data):\n",
    "    print(\"h2 has wrong shape\")\n",
    "if h3.shape != (D_3, n_data):\n",
    "    print(\"h3 has wrong shape\")\n",
    "if y.shape != (D_o, n_data):\n",
    "    print(\"Output has wrong shape\")\n",
    "\n",
    "# Print the inputs and outputs\n",
    "print(\"Input data points:\")\n",
    "print(x)\n",
    "print(\"Output data points:\")\n",
    "print(y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
